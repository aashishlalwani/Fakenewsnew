{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration and Analysis\n",
    "\n",
    "This notebook provides an initial exploration of the fake news dataset.\n",
    "\n",
    "## Objectives:\n",
    "- Load and examine the dataset\n",
    "- Understand data distribution\n",
    "- Identify missing values and data quality issues\n",
    "- Visualize key patterns\n",
    "- Generate insights for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace with your actual dataset path\n",
    "# Example datasets you can use:\n",
    "# 1. LIAR dataset: https://www.cs.ucsb.edu/~william/data/liar_dataset.zip\n",
    "# 2. Fake News Detection: https://www.kaggle.com/c/fake-news/data\n",
    "# 3. ISOT dataset: https://www.uvic.ca/engineering/ece/isot/datasets/\n",
    "\n",
    "# df = pd.read_csv('../data/raw/fake_news_dataset.csv')\n",
    "# print(f\"Dataset shape: {df.shape}\")\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Data Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "# df.info()\n",
    "# print(\"\\nColumn names:\")\n",
    "# print(df.columns.tolist())\n",
    "# print(\"\\nMissing values:\")\n",
    "# print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the distribution of fake vs real news\n",
    "# label_counts = df['label'].value_counts()\n",
    "# print(label_counts)\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# label_counts.plot(kind='bar')\n",
    "# plt.title('Distribution of Fake vs Real News')\n",
    "# plt.xlabel('Label')\n",
    "# plt.ylabel('Count')\n",
    "# plt.xticks(rotation=0)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text length patterns\n",
    "# df['text_length'] = df['text'].str.len()\n",
    "# df['word_count'] = df['text'].str.split().str.len()\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# # Character length distribution\n",
    "# df.boxplot(column='text_length', by='label', ax=ax1)\n",
    "# ax1.set_title('Text Length Distribution by Label')\n",
    "\n",
    "# # Word count distribution\n",
    "# df.boxplot(column='word_count', by='label', ax=ax2)\n",
    "# ax2.set_title('Word Count Distribution by Label')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word clouds for fake and real news\n",
    "# fake_text = ' '.join(df[df['label'] == 'FAKE']['text'])\n",
    "# real_text = ' '.join(df[df['label'] == 'REAL']['text'])\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "# # Fake news word cloud\n",
    "# fake_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(fake_text)\n",
    "# ax1.imshow(fake_wordcloud, interpolation='bilinear')\n",
    "# ax1.set_title('Most Frequent Words in Fake News', fontsize=16)\n",
    "# ax1.axis('off')\n",
    "\n",
    "# # Real news word cloud\n",
    "# real_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(real_text)\n",
    "# ax2.imshow(real_wordcloud, interpolation='bilinear')\n",
    "# ax2.set_title('Most Frequent Words in Real News', fontsize=16)\n",
    "# ax2.axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Most Common Words Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK data if needed\n",
    "# nltk.download('stopwords')\n",
    "# from nltk.corpus import stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# def get_common_words(text_series, n=20):\n",
    "#     \"\"\"Get most common words from text series\"\"\"\n",
    "#     all_words = []\n",
    "#     for text in text_series:\n",
    "#         words = text.lower().split()\n",
    "#         words = [word for word in words if word not in stop_words and word.isalpha()]\n",
    "#         all_words.extend(words)\n",
    "#     return Counter(all_words).most_common(n)\n",
    "\n",
    "# fake_common = get_common_words(df[df['label'] == 'FAKE']['text'])\n",
    "# real_common = get_common_words(df[df['label'] == 'REAL']['text'])\n",
    "\n",
    "# print(\"Most common words in FAKE news:\")\n",
    "# for word, count in fake_common[:10]:\n",
    "#     print(f\"{word}: {count}\")\n",
    "\n",
    "# print(\"\\nMost common words in REAL news:\")\n",
    "# for word, count in real_common[:10]:\n",
    "#     print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Insights and Next Steps\n",
    "\n",
    "Based on the exploration above, document your key findings:\n",
    "\n",
    "1. **Dataset Balance**: \n",
    "2. **Text Characteristics**: \n",
    "3. **Vocabulary Differences**: \n",
    "4. **Data Quality Issues**: \n",
    "5. **Preprocessing Needs**: \n",
    "\n",
    "### Recommended Next Steps:\n",
    "1. Clean and preprocess the text data\n",
    "2. Handle missing values appropriately\n",
    "3. Balance the dataset if needed\n",
    "4. Prepare features for modeling\n",
    "5. Split data into train/validation/test sets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}